{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: annoy in /opt/homebrew/lib/python3.11/site-packages (1.17.3)\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: PyPDF2 in /opt/homebrew/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: plotly in /opt/homebrew/lib/python3.11/site-packages (5.18.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/homebrew/lib/python3.11/site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.45.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/homebrew/lib/python3.11/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk transformers torch annoy seaborn matplotlib scikit-learn PyPDF2 plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aayushgupta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aayushgupta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import PyPDF2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn import DataParallel\n",
    "import torch\n",
    "from paper_processing_for_embeddings import preprocess_and_read \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_in_parallel(folder_path, n, num_workers=8):\n",
    "    all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    processed_files = all_files[:n]\n",
    "\n",
    "    with Pool(num_workers) as p:\n",
    "        results = list(tqdm(p.imap(preprocess_and_read, processed_files), total=len(processed_files)))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_words_batch(words):\n",
    "    inputs = tokenizer(words, padding=True, return_tensors='pt', truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the appropriate device\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.hidden_states[-1][:, 0, :].detach().cpu().numpy()\n",
    "\n",
    "def create_embeddings_from_preprocessed_data(preprocessed_data, batch_size):\n",
    "    all_embeddings = {}\n",
    "\n",
    "    for words, file_path in preprocessed_data:\n",
    "        unique_words = list(set(words))\n",
    "        cached_embeddings = defaultdict(lambda: None)\n",
    "\n",
    "        for i in range(0, len(unique_words), batch_size):\n",
    "            batch_words = unique_words[i:i + batch_size]\n",
    "            batch_embeddings = embed_words_batch(batch_words)\n",
    "\n",
    "            for word, embedding in zip(batch_words, batch_embeddings):\n",
    "                all_embeddings[word] = {\n",
    "                    'embedding': embedding,\n",
    "                    'file': file_path\n",
    "                }\n",
    "\n",
    "    return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snapshot_download' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m snapshot_download(repo_id\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPromptSystematicReview/Prompt_Systematic_Review_Dataset\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m                   repo_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m                   local_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./PapersDirectory\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                   allow_patterns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mpapers/*\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m      5\u001b[0m                   local_dir_use_symlinks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snapshot_download' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "snapshot_download(repo_id='PromptSystematicReview/Prompt_Systematic_Review_Dataset', \n",
    "                  repo_type='dataset', \n",
    "                  local_dir='./PapersDirectory', \n",
    "                  allow_patterns=['papers/*'], \n",
    "                  local_dir_use_symlinks=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def count_pdfs_in_directory(directory_path):\n",
    "    return len([f for f in os.listdir(directory_path) if f.endswith('.pdf') and os.path.isfile(os.path.join(directory_path, f))])\n",
    "\n",
    "# Set the path to the directory containing the papers\n",
    "papers_path = './PapersDirectory/papers'\n",
    "batch_size = 25\n",
    "\n",
    "# Count the number of PDFs in the directory to process all available papers\n",
    "num_papers_to_process = count_pdfs_in_directory(papers_path)\n",
    "\n",
    "# Continue with processing\n",
    "preprocessed_data = process_pdfs_in_parallel(papers_path, num_papers_to_process)\n",
    "embeddings_dict = create_embeddings_from_preprocessed_data(preprocessed_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming embeddings_dict is your dictionary\n",
    "with open('vector_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_dict, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
