{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: annoy in /opt/homebrew/lib/python3.11/site-packages (1.17.3)\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: PyPDF2 in /opt/homebrew/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: plotly in /opt/homebrew/lib/python3.11/site-packages (5.18.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/homebrew/lib/python3.11/site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.45.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/homebrew/lib/python3.11/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aayushgupta/Library/Python/3.11/lib/python/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk transformers torch annoy seaborn matplotlib scikit-learn PyPDF2 plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aayushgupta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aayushgupta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import PyPDF2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn import DataParallel\n",
    "import torch\n",
    "from paper_processing_for_embeddings import preprocess_and_read \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_words_batch(words):\n",
    "    inputs = tokenizer(words, padding=True, return_tensors='pt', truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the appropriate device\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.hidden_states[-1][:, 0, :].detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_annoy_index(embeddings_dict):\n",
    "    f = list(embeddings_dict.values())[0]['embedding'].shape[0]\n",
    "    t = AnnoyIndex(f, 'angular')\n",
    "    for i, (word, data) in enumerate(embeddings_dict.items()):\n",
    "        t.add_item(i, data['embedding'])\n",
    "    t.build(10)\n",
    "    return t\n",
    "\n",
    "def query_similar_words(query, index, embeddings_dict, top_n=5):\n",
    "    query_embedding = embed_words_batch([query])[0]  # Embed the query word\n",
    "    nearest_ids = index.get_nns_by_vector(query_embedding, top_n)\n",
    "\n",
    "    similar_words_with_titles = []\n",
    "    for i in nearest_ids:\n",
    "        word = list(embeddings_dict.keys())[i]\n",
    "        title = embeddings_dict[word]['file'].split('/')[-1]  # Extract the file name\n",
    "        similar_words_with_titles.append((word, title))\n",
    "\n",
    "    return similar_words_with_titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_word_embeddings.pkl', 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Annoy index\n",
    "annoy_index = build_annoy_index(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'llm': [('crucial', 'multiparty goal tracking with llms comparing pretraining, finetuning, and prompt engineering.pdf'), ('2018', 'multiparty goal tracking with llms comparing pretraining, finetuning, and prompt engineering.pdf'), ('2016', 'promptner prompting for named entity recognition.pdf'), ('2012', 'multiparty goal tracking with llms comparing pretraining, finetuning, and prompt engineering.pdf'), ('david', 'multiparty goal tracking with llms comparing pretraining, finetuning, and prompt engineering.pdf'), ('1999', 'promptner prompting for named entity recognition.pdf'), ('2015', 'promptner prompting for named entity recognition.pdf'), ('1998', 'promptner prompting for named entity recognition.pdf'), ('2004', 'multiparty goal tracking with llms comparing pretraining, finetuning, and prompt engineering.pdf'), ('2017', 'promptner prompting for named entity recognition.pdf')]\n"
     ]
    }
   ],
   "source": [
    "# Query for similar words\n",
    "query_word = \"llm\"  # Replace with your query word\n",
    "similar_words = query_similar_words(query_word, annoy_index, embeddings_dict, top_n=10)\n",
    "print(f\"Words similar to '{query_word}': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def optimized_plot_embeddings(embeddings_dict, query, index, top_n=5):\n",
    "    # Extract embeddings and corresponding words\n",
    "    words, embeddings = zip(*[(word, data['embedding']) for word, data in embeddings_dict.items() if data['embedding'] is not None])\n",
    "\n",
    "    # Randomly sample 1000 embeddings\n",
    "    sample_size = min(100000, len(embeddings))  # Adjust sample size as needed\n",
    "    sampled_indices = random.sample(range(len(embeddings)), sample_size)\n",
    "    sampled_embeddings = [embeddings[i] for i in sampled_indices]\n",
    "    sampled_words = [words[i] for i in sampled_indices]\n",
    "\n",
    "    # Add the query embedding\n",
    "    query_embedding = embeddings_dict.get(query, {'embedding': None})['embedding']\n",
    "    if query_embedding is None:\n",
    "        query_embedding = embed_words_batch([query])[0]  # Replace with your actual embedding function\n",
    "\n",
    "    # Add nearest neighbors of the query\n",
    "    nearest_neighbors_indices = index.get_nns_by_vector(query_embedding, top_n)\n",
    "    nearest_neighbors_embeddings = [embeddings[i] for i in nearest_neighbors_indices]\n",
    "    nearest_neighbors_words = [words[i] for i in nearest_neighbors_indices]\n",
    "\n",
    "    # Combine sampled embeddings, query, and nearest neighbors\n",
    "    extended_embeddings = np.vstack(sampled_embeddings + [query_embedding] + nearest_neighbors_embeddings)\n",
    "\n",
    "    # TSNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(extended_embeddings)\n",
    "\n",
    "    # KMeans for cluster-based coloring\n",
    "    num_clusters = 5  # Adjust as needed\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    clusters = kmeans.fit_predict(tsne_results[:-1-top_n])  # Apply KMeans to the sampled embeddings only\n",
    "\n",
    "    # Plotting\n",
    "    sns.set(style='whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Scatter plot for sampled embeddings with cluster coloring\n",
    "    scatter = plt.scatter(tsne_results[:-1-top_n, 0], tsne_results[:-1-top_n, 1], c=clusters, alpha=0.5, cmap='viridis')\n",
    "\n",
    "    # Highlight the query\n",
    "    plt.scatter(tsne_results[-1-top_n, 0], tsne_results[-1-top_n, 1], color='blue', marker='X', s=100)\n",
    "    plt.annotate(query, (tsne_results[-1-top_n, 0], tsne_results[-1-top_n, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    # Highlight nearest neighbors\n",
    "    for i, word in enumerate(nearest_neighbors_words):\n",
    "        nn_index = -top_n + i\n",
    "        plt.scatter(tsne_results[nn_index, 0], tsne_results[nn_index, 1], color='red', edgecolors='black', s=100)\n",
    "        plt.annotate(word, (tsne_results[nn_index, 0], tsne_results[nn_index, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    plt.title('t-SNE Visualization with Cluster-Based Coloring, Query, and Neighbors')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.colorbar(scatter)  # Show color scale for clusters\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Make sure to replace 'embed_words_batch', 'embeddings_dict', and 'annoy_index' with your actual variables\n",
    "optimized_plot_embeddings(embeddings_dict, \"gpt4\", annoy_index, top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_embeddings_3d_interactive(embeddings_dict, query, index, top_n=5):\n",
    "    words, embeddings = zip(*[(word, data['embedding']) for word, data in embeddings_dict.items() if data['embedding'] is not None])\n",
    "    query_embedding = embeddings_dict.get(query, {'embedding': None})['embedding']\n",
    "    \n",
    "    if query_embedding is None:\n",
    "        query_embedding = embed_words_batch([query])[0]\n",
    "\n",
    "    extended_embeddings = np.vstack(embeddings + (query_embedding,))\n",
    "\n",
    "    tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(extended_embeddings)\n",
    "\n",
    "    # Convert Seaborn colors to RGB format acceptable by Plotly\n",
    "    colors = sns.color_palette(\"hsv\", len(tsne_results))\n",
    "    colors_rgb = ['rgb' + str(tuple(int(x*255) for x in color)) for color in colors]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot all points with unique colors\n",
    "    for i, (x, y, z) in enumerate(tsne_results[:-1]):\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[x],\n",
    "            y=[y],\n",
    "            z=[z],\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color=colors_rgb[i], opacity=0.5)\n",
    "        ))\n",
    "\n",
    "    # Highlight and label the query point\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[tsne_results[-1, 0]],\n",
    "        y=[tsne_results[-1, 1]],\n",
    "        z=[tsne_results[-1, 2]],\n",
    "        mode='markers+text',\n",
    "        marker=dict(size=8, color='blue'),\n",
    "        text=[query],\n",
    "        textposition=\"bottom center\"\n",
    "    ))\n",
    "\n",
    "    # Find, highlight, and label nearest neighbors\n",
    "    indices = index.get_nns_by_vector(query_embedding, top_n)\n",
    "    for i in indices:\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[tsne_results[i, 0]],\n",
    "            y=[tsne_results[i, 1]],\n",
    "            z=[tsne_results[i, 2]],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=8, color='red'),\n",
    "            text=[words[i]],\n",
    "            textposition=\"bottom center\"\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, b=0, t=0),\n",
    "        scene=dict(\n",
    "            xaxis_title='t-SNE Component 1',\n",
    "            yaxis_title='t-SNE Component 2',\n",
    "            zaxis_title='t-SNE Component 3'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Assuming embeddings_dict and annoy_index are already created\n",
    "# Example query word\n",
    "query_word = \"gpt4\"  # Replace with your query word\n",
    "\n",
    "# Plotting the embeddings in 3D with an interactive plot and unique colors\n",
    "plot_embeddings_3d_interactive(embeddings_dict, query_word, annoy_index, top_n=25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3 (main, Apr  7 2023, 21:05:46) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
