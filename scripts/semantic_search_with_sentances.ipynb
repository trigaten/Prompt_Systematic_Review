{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk transformers torch annoy seaborn matplotlib scikit-learn umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING THE FILE TEXT IN\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/Users/aayushgupta/Downloads/James Green.txt'\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    document = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING THE TEXT\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEGMENTING THE DOCUMENT\n",
    "\n",
    "#This is for sentance\n",
    "segments = sent_tokenize(document)\n",
    "\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Replace sentence segmentation with word tokenization\n",
    "# words = word_tokenize(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMBEDDING THE TEXT\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "embeddings = [embed_text(preprocess_text(seg)) for seg in segments]\n",
    "embeddings = np.array(embeddings).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOING THE QUERYING \n",
    "\n",
    "# Check the first embedding's shape\n",
    "print(embeddings[0].shape)\n",
    "\n",
    "# If the shape is not (embedding_size,), then adjust the embeddings\n",
    "embeddings = [e[0] if len(e.shape) > 1 else e for e in embeddings]\n",
    "\n",
    "f = embeddings[0].shape[0]  # The embedding size\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "\n",
    "for i, vec in enumerate(embeddings):\n",
    "    t.add_item(i, vec)\n",
    "\n",
    "t.build(10)\n",
    "t.save('test.ann')\n",
    "\n",
    "def search(query, index, top_n=10):\n",
    "    query_embedding = embed_text(preprocess_text(query))\n",
    "    query_embedding = query_embedding[0] if len(query_embedding.shape) > 1 else query_embedding\n",
    "    indices = index.get_nns_by_vector(query_embedding, top_n)\n",
    "    return [segments[i] for i in indices]\n",
    "\n",
    "\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('test.ann')\n",
    "\n",
    "search_results = search(\"Value Innovation Purpose\", u, top_n=5)\n",
    "for result in search_results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "# Function to embed text, search, and plot\n",
    "def search_and_plot(query, index, embeddings, segments, top_n=5):\n",
    "    # Embed the query\n",
    "    query_embedding = embed_text(preprocess_text(query))[0]\n",
    "    extended_embeddings = np.append(embeddings, [query_embedding], axis=0)\n",
    "\n",
    "    # Recalculate t-SNE for extended embeddings\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    extended_tsne_results = tsne.fit_transform(extended_embeddings)\n",
    "\n",
    "    # KMeans clustering on extended embeddings\n",
    "    kmeans = KMeans(n_clusters=5)\n",
    "    clusters = kmeans.fit_predict(extended_embeddings)\n",
    "\n",
    "    # Find nearest neighbors of the query\n",
    "    indices = index.get_nns_by_vector(query_embedding, top_n)\n",
    "\n",
    "    # Plotting\n",
    "    sns.set(style='whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot all points with cluster coloring\n",
    "    plt.scatter(extended_tsne_results[:-1, 0], extended_tsne_results[:-1, 1], c=clusters[:-1], alpha=0.5, cmap='viridis')\n",
    "\n",
    "    # Highlight the query point\n",
    "    plt.scatter(extended_tsne_results[-1, 0], extended_tsne_results[-1, 1], color='blue', marker='X')\n",
    "\n",
    "    # Highlight nearest neighbors with red outline\n",
    "    for i in indices:\n",
    "        plt.scatter(extended_tsne_results[i, 0], extended_tsne_results[i, 1], facecolors='none', edgecolors='red', s=100)\n",
    "        plt.annotate(\n",
    "            segments[i][:50] + '...' if len(segments[i]) > 50 else segments[i],\n",
    "            (extended_tsne_results[i, 0], extended_tsne_results[i, 1]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0,10),\n",
    "            ha='center'\n",
    "        )\n",
    "\n",
    "    # Label the query\n",
    "    plt.annotate(\"Query\", (extended_tsne_results[-1, 0], extended_tsne_results[-1, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    plt.title('t-SNE Visualization with Query and Nearest Neighbors')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.show()\n",
    "\n",
    "# Define and load the Annoy index\n",
    "f = embeddings[0].shape[0]\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('test.ann')  # Assuming 'test.ann' is your pre-built Annoy index file\n",
    "\n",
    "# Example usage\n",
    "search_query = \"Value Innovation Purpose\"\n",
    "search_and_plot(search_query, u, embeddings, segments, top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3 (main, Apr  7 2023, 21:05:46) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
