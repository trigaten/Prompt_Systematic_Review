{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk transformers torch annoy seaborn matplotlib scikit-learn PyPDF2 plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import PyPDF2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn import DataParallel\n",
    "import torch\n",
    "from paper_processing_for_embeddings import preprocess_and_read \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_words_batch(words):\n",
    "    inputs = tokenizer(words, padding=True, return_tensors='pt', truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the appropriate device\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.hidden_states[-1][:, 0, :].detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_annoy_index(embeddings_dict):\n",
    "    f = list(embeddings_dict.values())[0]['embedding'].shape[0]\n",
    "    t = AnnoyIndex(f, 'angular')\n",
    "    for i, (word, data) in enumerate(embeddings_dict.items()):\n",
    "        t.add_item(i, data['embedding'])\n",
    "    t.build(10)\n",
    "    return t\n",
    "\n",
    "def query_similar_words(query, index, embeddings_dict, top_n=5):\n",
    "    query_embedding = embed_words_batch([query])[0]  # Embed the query word\n",
    "    nearest_ids = index.get_nns_by_vector(query_embedding, top_n)\n",
    "\n",
    "    similar_words_with_titles = []\n",
    "    for i in nearest_ids:\n",
    "        word = list(embeddings_dict.keys())[i]\n",
    "        title = embeddings_dict[word]['file'].split('/')[-1]  # Extract the file name\n",
    "        similar_words_with_titles.append((word, title))\n",
    "\n",
    "    return similar_words_with_titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_embeddings.pkl', 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Annoy index\n",
    "annoy_index = build_annoy_index(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Enhances normalization of text by lowercasing, replacing hyphens and underscores, \n",
    "    and removing non-alphanumeric characters.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[\\-\\_]', ' ', text)  # Replace hyphens and underscores with spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    text = ' '.join(text.split())  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "def user_approval(technique):\n",
    "    \"\"\"\n",
    "    Asks for user approval for a new technique.\n",
    "    \"\"\"\n",
    "    response = input(f\"Approve technique '{technique}'? (y/n): \").strip().lower()\n",
    "    return response == 'y'\n",
    "\n",
    "\n",
    "def find_nearest_neighbors(techniques, embeddings_dict, annoy_index, depth=0, max_depth=10):\n",
    "    \"\"\"\n",
    "    Recursively finds nearest neighbors for a list of techniques up to a certain depth.\n",
    "    Each new technique requires user approval.\n",
    "    \"\"\"\n",
    "    if depth >= max_depth:\n",
    "        print(f\"Maximum depth reached: {max_depth}\")\n",
    "        return techniques\n",
    "\n",
    "    new_techniques = set()\n",
    "    for technique in techniques:\n",
    "        similar_words = query_similar_words(technique, annoy_index, embeddings_dict, top_n=10)\n",
    "        for word, _ in similar_words:\n",
    "            normalized_word = normalize_text(word)\n",
    "            if normalized_word not in techniques:\n",
    "                # Present the normalized word for user approval\n",
    "                if user_approval(normalized_word):\n",
    "                    new_techniques.add(normalized_word)\n",
    "\n",
    "    # Check for new techniques\n",
    "    if not new_techniques:\n",
    "        print(\"No new approved techniques found\")\n",
    "        return techniques\n",
    "\n",
    "    # Add new techniques and recurse\n",
    "    updated_techniques = techniques.union(new_techniques)\n",
    "    print(f\"Depth {depth}: Found {len(new_techniques)} new approved techniques\")\n",
    "    return find_nearest_neighbors(updated_techniques, embeddings_dict, annoy_index, depth+1, max_depth)\n",
    "\n",
    "# Example usage\n",
    "initial_techniques = {\"chain-of-thought\", \"few-shot\"}  # Replace with your initial set of techniques\n",
    "max_depth = 5  # You can adjust this to control the recursion depth\n",
    "all_techniques = find_nearest_neighbors(initial_techniques, embeddings_dict, annoy_index, max_depth=max_depth)\n",
    "\n",
    "print(\"All approved techniques found:\")\n",
    "for technique in all_techniques:\n",
    "    print(technique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
