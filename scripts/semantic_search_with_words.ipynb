{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk transformers torch annoy seaborn matplotlib scikit-learn PyPDF2 plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aayushgupta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aayushgupta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import PyPDF2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn import DataParallel\n",
    "import torch\n",
    "from paper_processing_for_embeddings import preprocess_and_read \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# If multiple GPUs are available, use DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_words_batch(words):\n",
    "    inputs = tokenizer(words, padding=True, return_tensors='pt', truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the appropriate device\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.hidden_states[-1][:, 0, :].detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_annoy_index(embeddings_dict):\n",
    "    f = list(embeddings_dict.values())[0]['embedding'].shape[0]\n",
    "    t = AnnoyIndex(f, 'angular')\n",
    "    for i, (word, data) in enumerate(embeddings_dict.items()):\n",
    "        t.add_item(i, data['embedding'])\n",
    "    t.build(10)\n",
    "    return t\n",
    "\n",
    "def query_similar_words(query, index, embeddings_dict, top_n=5):\n",
    "    query_embedding = embed_words_batch([query])[0]  # Embed the query word\n",
    "    nearest_ids = index.get_nns_by_vector(query_embedding, top_n)\n",
    "\n",
    "    similar_words_with_titles = []\n",
    "    for i in nearest_ids:\n",
    "        word = list(embeddings_dict.keys())[i]\n",
    "        title = embeddings_dict[word]['file'].split('/')[-1]  # Extract the file name\n",
    "        similar_words_with_titles.append((word, title))\n",
    "\n",
    "    return similar_words_with_titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_embeddings.pkl', 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Annoy index\n",
    "annoy_index = build_annoy_index(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for similar words\n",
    "query_word = \"Chain-of-Thought\"  # Replace with your query word\n",
    "similar_words = query_similar_words(query_word, annoy_index, embeddings_dict, top_n=50)\n",
    "print(f\"Words similar to '{query_word}': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def optimized_plot_embeddings(embeddings_dict, query, index, top_n=5):\n",
    "    # Extract embeddings and corresponding words\n",
    "    words, embeddings = zip(*[(word, data['embedding']) for word, data in embeddings_dict.items() if data['embedding'] is not None])\n",
    "\n",
    "    # Randomly sample 1000 embeddings\n",
    "    sample_size = min(100000, len(embeddings))  # Adjust sample size as needed\n",
    "    sampled_indices = random.sample(range(len(embeddings)), sample_size)\n",
    "    sampled_embeddings = [embeddings[i] for i in sampled_indices]\n",
    "    sampled_words = [words[i] for i in sampled_indices]\n",
    "\n",
    "    # Add the query embedding\n",
    "    query_embedding = embeddings_dict.get(query, {'embedding': None})['embedding']\n",
    "    if query_embedding is None:\n",
    "        query_embedding = embed_words_batch([query])[0]  # Replace with your actual embedding function\n",
    "\n",
    "    # Add nearest neighbors of the query\n",
    "    nearest_neighbors_indices = index.get_nns_by_vector(query_embedding, top_n)\n",
    "    nearest_neighbors_embeddings = [embeddings[i] for i in nearest_neighbors_indices]\n",
    "    nearest_neighbors_words = [words[i] for i in nearest_neighbors_indices]\n",
    "\n",
    "    # Combine sampled embeddings, query, and nearest neighbors\n",
    "    extended_embeddings = np.vstack(sampled_embeddings + [query_embedding] + nearest_neighbors_embeddings)\n",
    "\n",
    "    # TSNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(extended_embeddings)\n",
    "\n",
    "    # KMeans for cluster-based coloring\n",
    "    num_clusters = 5  # Adjust as needed\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    clusters = kmeans.fit_predict(tsne_results[:-1-top_n])  # Apply KMeans to the sampled embeddings only\n",
    "\n",
    "    # Plotting\n",
    "    sns.set(style='whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Scatter plot for sampled embeddings with cluster coloring\n",
    "    scatter = plt.scatter(tsne_results[:-1-top_n, 0], tsne_results[:-1-top_n, 1], c=clusters, alpha=0.5, cmap='viridis')\n",
    "\n",
    "    # Highlight the query\n",
    "    plt.scatter(tsne_results[-1-top_n, 0], tsne_results[-1-top_n, 1], color='blue', marker='X', s=100)\n",
    "    plt.annotate(query, (tsne_results[-1-top_n, 0], tsne_results[-1-top_n, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    # Highlight nearest neighbors\n",
    "    for i, word in enumerate(nearest_neighbors_words):\n",
    "        nn_index = -top_n + i\n",
    "        plt.scatter(tsne_results[nn_index, 0], tsne_results[nn_index, 1], color='red', edgecolors='black', s=100)\n",
    "        plt.annotate(word, (tsne_results[nn_index, 0], tsne_results[nn_index, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    plt.title('t-SNE Visualization with Cluster-Based Coloring, Query, and Neighbors')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.colorbar(scatter)  # Show color scale for clusters\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Make sure to replace 'embed_words_batch', 'embeddings_dict', and 'annoy_index' with your actual variables\n",
    "optimized_plot_embeddings(embeddings_dict, \"gpt4\", annoy_index, top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_embeddings_3d_interactive(embeddings_dict, query, index, top_n=5):\n",
    "    words, embeddings = zip(*[(word, data['embedding']) for word, data in embeddings_dict.items() if data['embedding'] is not None])\n",
    "    query_embedding = embeddings_dict.get(query, {'embedding': None})['embedding']\n",
    "    \n",
    "    if query_embedding is None:\n",
    "        query_embedding = embed_words_batch([query])[0]\n",
    "\n",
    "    extended_embeddings = np.vstack(embeddings + (query_embedding,))\n",
    "\n",
    "    tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(extended_embeddings)\n",
    "\n",
    "    # Convert Seaborn colors to RGB format acceptable by Plotly\n",
    "    colors = sns.color_palette(\"hsv\", len(tsne_results))\n",
    "    colors_rgb = ['rgb' + str(tuple(int(x*255) for x in color)) for color in colors]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot all points with unique colors\n",
    "    for i, (x, y, z) in enumerate(tsne_results[:-1]):\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[x],\n",
    "            y=[y],\n",
    "            z=[z],\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color=colors_rgb[i], opacity=0.5)\n",
    "        ))\n",
    "\n",
    "    # Highlight and label the query point\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[tsne_results[-1, 0]],\n",
    "        y=[tsne_results[-1, 1]],\n",
    "        z=[tsne_results[-1, 2]],\n",
    "        mode='markers+text',\n",
    "        marker=dict(size=8, color='blue'),\n",
    "        text=[query],\n",
    "        textposition=\"bottom center\"\n",
    "    ))\n",
    "\n",
    "    # Find, highlight, and label nearest neighbors\n",
    "    indices = index.get_nns_by_vector(query_embedding, top_n)\n",
    "    for i in indices:\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[tsne_results[i, 0]],\n",
    "            y=[tsne_results[i, 1]],\n",
    "            z=[tsne_results[i, 2]],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=8, color='red'),\n",
    "            text=[words[i]],\n",
    "            textposition=\"bottom center\"\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, b=0, t=0),\n",
    "        scene=dict(\n",
    "            xaxis_title='t-SNE Component 1',\n",
    "            yaxis_title='t-SNE Component 2',\n",
    "            zaxis_title='t-SNE Component 3'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Assuming embeddings_dict and annoy_index are already created\n",
    "# Example query word\n",
    "query_word = \"gpt4\"  # Replace with your query word\n",
    "\n",
    "# Plotting the embeddings in 3D with an interactive plot and unique colors\n",
    "plot_embeddings_3d_interactive(embeddings_dict, query_word, annoy_index, top_n=25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3 (main, Apr  7 2023, 21:05:46) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
