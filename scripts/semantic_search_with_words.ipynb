{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk transformers torch annoy seaborn matplotlib scikit-learn PyPDF2 plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from annoy import AnnoyIndex\n",
    "import PyPDF2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING THE FILE TEXT IN\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \" \".join([page.extract_text() for page in pdf_reader.pages if page.extract_text() is not None])\n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING THE TEXT\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return filtered_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# Modify the embedding function for batch processing\n",
    "def embed_words_batch(words):\n",
    "    inputs = tokenizer(words, padding=True, return_tensors='pt', truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.hidden_states[-1][:, 0, :].detach().numpy()\n",
    "\n",
    "# Creating embeddings with caching and batch processing\n",
    "def create_embedding_dictionary_batch(file_path, batch_size=10):\n",
    "    document_text = read_pdf(file_path)\n",
    "    words = preprocess_text(document_text)\n",
    "    unique_words = list(set(words))  # Unique words for caching\n",
    "\n",
    "    # Caching embeddings\n",
    "    cached_embeddings = defaultdict(lambda: None)\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    for i in range(0, len(unique_words), batch_size):\n",
    "        batch_words = unique_words[i:i+batch_size]\n",
    "        batch_embeddings = embed_words_batch(batch_words)\n",
    "\n",
    "        for word, embedding in zip(batch_words, batch_embeddings):\n",
    "            cached_embeddings[word] = embedding\n",
    "\n",
    "    for word in words:\n",
    "        embeddings_dict[word] = {\n",
    "            'embedding': cached_embeddings[word],\n",
    "            'file': file_path\n",
    "        }\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def process_multiple_pdfs(folder_path, n, batch=10):\n",
    "    all_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    processed_files = all_files[:n]  # Process only the first n files\n",
    "\n",
    "    all_embeddings = {}\n",
    "    for file in tqdm(processed_files, desc=\"Processing PDFs\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            embeddings_dict = create_embedding_dictionary_batch(file_path, batch)\n",
    "            all_embeddings.update(embeddings_dict)\n",
    "        except Exception as e:  # Catching a more general exception\n",
    "            print(f\"Error reading PDF: {file}. Skipping this file.\")\n",
    "            continue\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_annoy_index(embeddings_dict):\n",
    "    f = list(embeddings_dict.values())[0]['embedding'].shape[0]\n",
    "    t = AnnoyIndex(f, 'angular')\n",
    "    for i, (word, data) in enumerate(embeddings_dict.items()):\n",
    "        t.add_item(i, data['embedding'])\n",
    "    t.build(10)\n",
    "    return t\n",
    "\n",
    "def query_similar_words(query, index, embeddings_dict, top_n=5):\n",
    "    query_embedding = embed_words_batch([query])[0]  # Embed the query word\n",
    "    nearest_ids = index.get_nns_by_vector(query_embedding, top_n)\n",
    "\n",
    "    similar_words_with_titles = []\n",
    "    for i in nearest_ids:\n",
    "        word = list(embeddings_dict.keys())[i]\n",
    "        title = embeddings_dict[word]['file'].split('/')[-1]  # Extract the file name\n",
    "        similar_words_with_titles.append((word, title))\n",
    "\n",
    "    return similar_words_with_titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/aayushgupta/Desktop/PapersDirectory/papers'\n",
    "num_papers_to_process = 100  # Replace with the number of papers you want to process\n",
    "\n",
    "embeddings_dict = process_multiple_pdfs(folder_path, num_papers_to_process, 40)\n",
    "\n",
    "\n",
    "# # Create embeddings dictionary\n",
    "# filepath = 'path'\n",
    "# embeddings_dict = create_embedding_dictionary_batch(file_path, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming embeddings_dict is your dictionary\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings.pkl', 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Annoy index\n",
    "annoy_index = build_annoy_index(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for similar words\n",
    "query_word = \"few-shot\"  # Replace with your query word\n",
    "similar_words = query_similar_words(query_word, annoy_index, embeddings_dict, top_n=25)\n",
    "print(f\"Words similar to '{query_word}': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def optimized_plot_embeddings(embeddings_dict, query, index, top_n=5):\n",
    "    # Extract existing embeddings and the query embedding from the dictionary\n",
    "    words, embeddings = zip(*[(word, data['embedding']) for word, data in embeddings_dict.items() if data['embedding'] is not None])\n",
    "    query_embedding = embeddings_dict.get(query, {'embedding': None})['embedding']\n",
    "    \n",
    "    # Handle case where query embedding is not pre-computed\n",
    "    if query_embedding is None:\n",
    "        query_embedding = embed_words_batch([query])[0]\n",
    "\n",
    "    extended_embeddings = np.vstack(embeddings + (query_embedding,))\n",
    "    \n",
    "    # TSNE for dimensionality reduction (can be cached for the same dataset)\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(extended_embeddings)\n",
    "\n",
    "    # KMeans for clustering (can be cached for the same dataset)\n",
    "    kmeans = KMeans(n_clusters=5)\n",
    "    clusters = kmeans.fit_predict(extended_embeddings)\n",
    "\n",
    "    # Find nearest neighbors using the pre-built index\n",
    "    indices = index.get_nns_by_vector(query_embedding, top_n)\n",
    "\n",
    "    # Plotting\n",
    "    sns.set(style='whitegrid')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot all points with cluster coloring\n",
    "    plt.scatter(tsne_results[:-1, 0], tsne_results[:-1, 1], c=clusters[:-1], alpha=0.5, cmap='viridis')\n",
    "\n",
    "    # Highlight the query point\n",
    "    plt.scatter(tsne_results[-1, 0], tsne_results[-1, 1], color='blue', marker='X')\n",
    "\n",
    "    # Highlight nearest neighbors with red outline\n",
    "    for i in indices:\n",
    "        plt.scatter(tsne_results[i, 0], tsne_results[i, 1], facecolors='none', edgecolors='red', s=100)\n",
    "        plt.annotate(words[i], (tsne_results[i, 0], tsne_results[i, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    # Label the query\n",
    "    plt.annotate(\"Query\", (tsne_results[-1, 0], tsne_results[-1, 1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    plt.title('Optimized t-SNE Visualization of Word Embeddings with Query and Nearest Neighbors')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example query word\n",
    "query_word = \"gpt4\"  # Replace with your query word\n",
    "\n",
    "# Plotting the embeddings assuming all previous steps have been completed\n",
    "optimized_plot_embeddings(embeddings_dict, query_word, annoy_index, top_n=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_embeddings_3d_interactive(embeddings_dict, query, index, top_n=5):\n",
    "    words, embeddings = zip(*[(word, data['embedding']) for word, data in embeddings_dict.items() if data['embedding'] is not None])\n",
    "    query_embedding = embeddings_dict.get(query, {'embedding': None})['embedding']\n",
    "    \n",
    "    if query_embedding is None:\n",
    "        query_embedding = embed_words_batch([query])[0]\n",
    "\n",
    "    extended_embeddings = np.vstack(embeddings + (query_embedding,))\n",
    "\n",
    "    tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(extended_embeddings)\n",
    "\n",
    "    # Convert Seaborn colors to RGB format acceptable by Plotly\n",
    "    colors = sns.color_palette(\"hsv\", len(tsne_results))\n",
    "    colors_rgb = ['rgb' + str(tuple(int(x*255) for x in color)) for color in colors]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot all points with unique colors\n",
    "    for i, (x, y, z) in enumerate(tsne_results[:-1]):\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[x],\n",
    "            y=[y],\n",
    "            z=[z],\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color=colors_rgb[i], opacity=0.5)\n",
    "        ))\n",
    "\n",
    "    # Highlight and label the query point\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[tsne_results[-1, 0]],\n",
    "        y=[tsne_results[-1, 1]],\n",
    "        z=[tsne_results[-1, 2]],\n",
    "        mode='markers+text',\n",
    "        marker=dict(size=8, color='blue'),\n",
    "        text=[query],\n",
    "        textposition=\"bottom center\"\n",
    "    ))\n",
    "\n",
    "    # Find, highlight, and label nearest neighbors\n",
    "    indices = index.get_nns_by_vector(query_embedding, top_n)\n",
    "    for i in indices:\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[tsne_results[i, 0]],\n",
    "            y=[tsne_results[i, 1]],\n",
    "            z=[tsne_results[i, 2]],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=8, color='red'),\n",
    "            text=[words[i]],\n",
    "            textposition=\"bottom center\"\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, b=0, t=0),\n",
    "        scene=dict(\n",
    "            xaxis_title='t-SNE Component 1',\n",
    "            yaxis_title='t-SNE Component 2',\n",
    "            zaxis_title='t-SNE Component 3'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Assuming embeddings_dict and annoy_index are already created\n",
    "# Example query word\n",
    "query_word = \"gpt4\"  # Replace with your query word\n",
    "\n",
    "# Plotting the embeddings in 3D with an interactive plot and unique colors\n",
    "plot_embeddings_3d_interactive(embeddings_dict, query_word, annoy_index, top_n=25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
